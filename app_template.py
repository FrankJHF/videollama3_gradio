# å¯¼å…¥osåº“ï¼Œç”¨äºå¤„ç†æ–‡ä»¶å’Œç›®å½•è·¯å¾„
import os
# ä»osåº“ä¸­å¯¼å…¥pathæ¨¡å—ï¼Œå¹¶é‡å‘½åä¸ºospï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
import os.path as osp

# å¯¼å…¥gradioåº“ï¼Œç”¨äºåˆ›å»ºWeb UIç•Œé¢
import gradio as gr
# å¯¼å…¥spacesï¼Œç”¨äºåœ¨Hugging Face Spacesä¸Šè¿›è¡Œéƒ¨ç½²ä¼˜åŒ–ï¼Œå¦‚æ­¤å¤„çš„GPUæ—¶é•¿æ§åˆ¶
import spaces
# å¯¼å…¥torchåº“ï¼ŒPyTorchçš„æ ¸å¿ƒåº“
import torch
# ä»threadingåº“å¯¼å…¥Threadï¼Œç”¨äºåœ¨åå°è¿è¡Œæ¨¡å‹ç”Ÿæˆï¼Œé¿å…UIé˜»å¡
from threading import Thread
# ä»transformersåº“å¯¼å…¥æ‰€éœ€çš„ç±»ï¼š
# AutoModelForCausalLM: ç”¨äºåŠ è½½é¢„è®­ç»ƒçš„å› æœè¯­è¨€æ¨¡å‹
# AutoProcessor: ç”¨äºåŠ è½½ä¸æ¨¡å‹åŒ¹é…çš„é¢„å¤„ç†å™¨ï¼Œå¤„ç†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ç­‰å¤šç§è¾“å…¥
# TextIteratorStreamer: ç”¨äºå®ç°æµå¼æ–‡æœ¬è¾“å‡º
from transformers import AutoModelForCausalLM, AutoProcessor, TextIteratorStreamer


# å®šä¹‰Gradioç•Œé¢é¡¶éƒ¨çš„HTMLå†…å®¹ï¼ŒåŒ…å«é¡¹ç›®æ ‡é¢˜ã€Logoã€Githubé“¾æ¥ã€è®ºæ–‡é“¾æ¥ç­‰
HEADER = ("""
<div style="display: flex; justify-content: center; align-items: center; text-align: center;">
  <a href="https://github.com/DAMO-NLP-SG/VideoLLaMA3" style="margin-right: 20px; text-decoration: none; display: flex; align-items: center;">
    <img src="https://github.com/DAMO-NLP-SG/VideoLLaMA3/blob/main/assets/logo.png?raw=true" alt="VideoLLaMA 3 ğŸ”¥ğŸš€ğŸ”¥" style="max-width: 120px; height: auto;">
  </a>
  <div>
    <h1>VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding</h1>
    <h5 style="margin: 0;">If this demo please you, please give us a star â­ on Github or ğŸ’– on this space.</h5>
  </div>
</div>

<div style="display: flex; justify-content: center; margin-top: 10px;">
  <a href="https://github.com/DAMO-NLP-SG/VideoLLaMA3"><img src='https://img.shields.io/badge/Github-VideoLLaMA3-9C276A' style="margin-right: 5px;"></a>
  <a href="https://arxiv.org/pdf/2501.13106"><img src="https://img.shields.io/badge/Arxiv-2501.13106-AD1C18" style="margin-right: 5px;"></a>
  <a href="https://huggingface.co/collections/DAMO-NLP-SG/videollama3-678cdda9281a0e32fe79af15"><img src="https://img.shields.io/badge/ğŸ¤—-Checkpoints-ED5A22.svg" style="margin-right: 5px;"></a>
  <a href="https://github.com/DAMO-NLP-SG/VideoLLaMA3/stargazers"><img src="https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA3.svg?style=social"></a>
</div>
""")

# è®¾ç½®æ¨¡å‹è¿è¡Œçš„è®¾å¤‡ï¼Œä¼˜å…ˆä½¿ç”¨CUDAï¼ˆNVIDIA GPUï¼‰
device = "cuda"
# åŠ è½½é¢„è®­ç»ƒçš„VideoLLaMA3æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "DAMO-NLP-SG/VideoLLaMA3-2B",  # æ¨¡å‹åœ¨Hugging Face Hubä¸Šçš„åç§°
    trust_remote_code=True,  # å…è®¸æ‰§è¡Œæ¨¡å‹ä»“åº“ä¸­çš„è‡ªå®šä¹‰ä»£ç 
    torch_dtype=torch.bfloat16,  # ä½¿ç”¨bfloat16æ•°æ®ç±»å‹ï¼Œä»¥èŠ‚çœæ˜¾å­˜å¹¶åŠ é€Ÿè®¡ç®—
    attn_implementation="flash_attention_2",  # ä½¿ç”¨Flash Attention 2ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æå‡é€Ÿåº¦å’Œæ•ˆç‡
)
# å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Š
model.to(device)
# åŠ è½½ä¸æ¨¡å‹é…å¥—çš„é¢„å¤„ç†å™¨
processor = AutoProcessor.from_pretrained("DAMO-NLP-SG/VideoLLaMA3-7B", trust_remote_code=True)


# å®šä¹‰å­˜æ”¾ç¤ºä¾‹æ–‡ä»¶çš„ç›®å½•
example_dir = "./examples"
# å®šä¹‰æ”¯æŒçš„å›¾ç‰‡å’Œè§†é¢‘æ–‡ä»¶æ ¼å¼
image_formats = ("png", "jpg", "jpeg")
video_formats = ("mp4",)

# åˆå§‹åŒ–ç”¨äºå­˜å‚¨ç¤ºä¾‹æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨
image_examples, video_examples = [], []
# å¦‚æœç¤ºä¾‹ç›®å½•å­˜åœ¨ï¼Œåˆ™åŠ è½½å…¶ä¸­çš„æ–‡ä»¶
if example_dir is not None:
    # è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å
    example_files = [
        osp.join(example_dir, f) for f in os.listdir(example_dir)
    ]
    # éå†æ–‡ä»¶ï¼Œæ ¹æ®åç¼€åå°†å…¶åˆ†ç±»åˆ°å›¾ç‰‡æˆ–è§†é¢‘ç¤ºä¾‹åˆ—è¡¨ä¸­
    for example_file in example_files:
        if example_file.endswith(image_formats):
            image_examples.append([example_file])
        elif example_file.endswith(video_formats):
            video_examples.append([example_file])


# å®šä¹‰è§†é¢‘ä¸Šä¼ åçš„å›è°ƒå‡½æ•°
def _on_video_upload(messages, video):
        """
        å½“ç”¨æˆ·ä¸Šä¼ è§†é¢‘æ—¶ï¼Œæ­¤å‡½æ•°è¢«è°ƒç”¨ã€‚
        å®ƒä¼šå°†è§†é¢‘è·¯å¾„æ·»åŠ åˆ°ä¸€ä¸ªä»£è¡¨å¯¹è¯å†å²çš„åˆ—è¡¨ä¸­ã€‚

        å‚æ•°:
            messages (list): å½“å‰çš„å¯¹è¯å†å²åˆ—è¡¨ã€‚
            video (str): ä¸Šä¼ è§†é¢‘çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€‚

        è¿”å›:
            tuple: æ›´æ–°åçš„å¯¹è¯å†å²åˆ—è¡¨å’Œä¸€ä¸ªNoneå€¼ï¼ˆç”¨äºæ¸…ç©ºè§†é¢‘ä¸Šä¼ ç»„ä»¶ï¼‰ã€‚
        """
        if video is not None:
            # messages.append({"role": "user", "content": gr.Video(video)})
            # å°†è§†é¢‘è·¯å¾„å°è£…æˆä¸€ä¸ªå­—å…¸ï¼Œæ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            messages.append({"role": "user", "content": {"path": video}})
        return messages, None

# å®šä¹‰å›¾ç‰‡ä¸Šä¼ åçš„å›è°ƒå‡½æ•°
def _on_image_upload(messages, image):
    """
    å½“ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡æ—¶ï¼Œæ­¤å‡½æ•°è¢«è°ƒç”¨ã€‚
    å®ƒä¼šå°†å›¾ç‰‡è·¯å¾„æ·»åŠ åˆ°ä¸€ä¸ªä»£è¡¨å¯¹è¯å†å²çš„åˆ—è¡¨ä¸­ã€‚

    å‚æ•°:
        messages (list): å½“å‰çš„å¯¹è¯å†å²åˆ—è¡¨ã€‚
        image (str): ä¸Šä¼ å›¾ç‰‡çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€‚

    è¿”å›:
        tuple: æ›´æ–°åçš„å¯¹è¯å†å²åˆ—è¡¨å’Œä¸€ä¸ªNoneå€¼ï¼ˆç”¨äºæ¸…ç©ºå›¾ç‰‡ä¸Šä¼ ç»„ä»¶ï¼‰ã€‚
    """
    if image is not None:
        # messages.append({"role": "user", "content": gr.Image(image)})
        # å°†å›¾ç‰‡è·¯å¾„å°è£…æˆä¸€ä¸ªå­—å…¸ï¼Œæ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        messages.append({"role": "user", "content": {"path": image}})
    return messages, None

# å®šä¹‰æ–‡æœ¬æäº¤åçš„å›è°ƒå‡½æ•°
def _on_text_submit(messages, text):
    """
    å½“ç”¨æˆ·æäº¤æ–‡æœ¬æ—¶ï¼Œæ­¤å‡½æ•°è¢«è°ƒç”¨ã€‚
    å®ƒä¼šå°†æ–‡æœ¬æ¶ˆæ¯æ·»åŠ åˆ°ä¸€ä¸ªä»£è¡¨å¯¹è¯å†å²çš„åˆ—è¡¨ä¸­ã€‚

    å‚æ•°:
        messages (list): å½“å‰çš„å¯¹è¯å†å²åˆ—è¡¨ã€‚
        text (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ã€‚

    è¿”å›:
        tuple: æ›´æ–°åçš„å¯¹è¯å†å²åˆ—è¡¨å’Œä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼ˆç”¨äºæ¸…ç©ºæ–‡æœ¬è¾“å…¥æ¡†ï¼‰ã€‚
    """
    messages.append({"role": "user", "content": text})
    return messages, ""

# ä½¿ç”¨Hugging Face Spacesçš„è£…é¥°å™¨ï¼Œä¸ºè¯¥å‡½æ•°ç”³è¯·GPUèµ„æºï¼Œè¿è¡Œæ—¶é•¿é™åˆ¶ä¸º120ç§’
@spaces.GPU(duration=120)
def _predict(messages, input_text, do_sample, temperature, top_p, max_new_tokens,
             fps, max_frames):
    """
    æ ¸å¿ƒé¢„æµ‹å‡½æ•°ï¼Œç”¨äºç”Ÿæˆæ¨¡å‹çš„å›å¤ã€‚

    å‚æ•°:
        messages (list): å®Œæ•´çš„å¯¹è¯å†å²ã€‚
        input_text (str): ç”¨æˆ·åœ¨æ–‡æœ¬æ¡†ä¸­æœ€æ–°è¾“å…¥çš„æ–‡æœ¬ã€‚
        do_sample (bool): æ˜¯å¦ä½¿ç”¨é‡‡æ ·ç”Ÿæˆã€‚
        temperature (float): æ§åˆ¶ç”Ÿæˆæ–‡æœ¬éšæœºæ€§çš„æ¸©åº¦å‚æ•°ã€‚
        top_p (float): Top-p (nucleus) é‡‡æ ·çš„é˜ˆå€¼ã€‚
        max_new_tokens (int): ç”Ÿæˆæ–°tokençš„æœ€å¤§æ•°é‡ã€‚
        fps (int): å¤„ç†è§†é¢‘æ—¶æŠ½å–çš„å¸§ç‡ã€‚
        max_frames (int): å¤„ç†è§†é¢‘æ—¶æŠ½å–çš„æœ€å¤§å¸§æ•°ã€‚

    è¿”å›:
        generator: ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œé€ä¸ªtokenåœ°äº§ç”Ÿå›å¤ï¼Œå®ç°æµå¼è¾“å‡ºã€‚
    """
    # å¦‚æœæ–‡æœ¬æ¡†ä¸­æœ‰æ–°çš„è¾“å…¥ï¼Œå…ˆå°†å…¶åŠ å…¥å¯¹è¯å†å²
    if len(input_text) > 0:
        messages.append({"role": "user", "content": input_text})
    # åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äºä¼ é€’ç»™å¤„ç†å™¨
    new_messages = []
    # åˆå§‹åŒ–ä¸€ä¸ªå†…å®¹åˆ—è¡¨ï¼Œç”¨äºåˆå¹¶è¿ç»­çš„ç”¨æˆ·æ–‡æœ¬æ¶ˆæ¯
    contents = []
    # éå†åŸå§‹å¯¹è¯å†å²ï¼Œå°†å…¶è½¬æ¢ä¸ºæ¨¡å‹å¤„ç†å™¨æ‰€éœ€çš„æ ¼å¼
    for message in messages:
        if message["role"] == "assistant":
            # å¦‚æœé‡åˆ°åŠ©æ‰‹æ¶ˆæ¯ï¼Œå…ˆå°†ä¹‹å‰æ”¶é›†çš„ç”¨æˆ·å†…å®¹æ·»åŠ åˆ°æ–°æ¶ˆæ¯åˆ—è¡¨
            if len(contents):
                new_messages.append({"role": "user", "content": contents})
                contents = []
            # ç„¶åæ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
            new_messages.append(message)
        elif message["role"] == "user":
            # å¦‚æœæ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ¤æ–­å†…å®¹æ˜¯æ–‡æœ¬è¿˜æ˜¯æ–‡ä»¶
            if isinstance(message["content"], str):
                # å¦‚æœæ˜¯æ–‡æœ¬ï¼Œæ·»åŠ åˆ°å½“å‰å†…å®¹åˆ—è¡¨
                contents.append(message["content"])
            else:
                # å¦‚æœæ˜¯æ–‡ä»¶ï¼ˆè§†é¢‘æˆ–å›¾ç‰‡ï¼‰ï¼Œæå–è·¯å¾„
                media_path = message["content"]["path"]
                if media_path.endswith(video_formats):
                    # å¦‚æœæ˜¯è§†é¢‘ï¼Œæ„å»ºè§†é¢‘æè¿°å­—å…¸
                    contents.append({"type": "video", "video": {"video_path": media_path, "fps": fps, "max_frames": max_frames}})
                elif media_path.endswith(image_formats):
                    # å¦‚æœæ˜¯å›¾ç‰‡ï¼Œæ„å»ºå›¾ç‰‡æè¿°å­—å…¸
                    contents.append({"type": "image", "image": {"image_path": media_path}})
                else:
                    raise ValueError(f"Unsupported media type: {media_path}")

    # å¦‚æœéå†ç»“æŸåä»æœ‰æœªå¤„ç†çš„ç”¨æˆ·å†…å®¹ï¼Œæ·»åŠ åˆ°æ–°æ¶ˆæ¯åˆ—è¡¨
    if len(contents):
        new_messages.append({"role": "user", "content": contents})

    # å¦‚æœæ²¡æœ‰ç”¨æˆ·è¾“å…¥ï¼Œæˆ–è€…æœ€åä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ç”¨æˆ·çš„ï¼Œåˆ™ä¸è¿›è¡Œç”Ÿæˆ
    if len(new_messages) == 0 or new_messages[-1]["role"] != "user":
        return messages

    # é…ç½®ç”Ÿæˆå‚æ•°
    generation_config = {
        "do_sample": do_sample,
        "temperature": temperature,
        "top_p": top_p,
        "max_new_tokens": max_new_tokens
    }

    # ä½¿ç”¨å¤„ç†å™¨å¤„ç†å¯¹è¯å†å²ï¼Œç”Ÿæˆæ¨¡å‹è¾“å…¥
    inputs = processor(
        conversation=new_messages,
        add_system_prompt=True,      # æ·»åŠ ç³»ç»Ÿæç¤º
        add_generation_prompt=True,  # æ·»åŠ ç”Ÿæˆæç¤º
        return_tensors="pt"          # è¿”å›PyTorchå¼ é‡
    )
    # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆGPUï¼‰
    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
    # å¦‚æœè¾“å…¥ä¸­åŒ…å«åƒç´ å€¼ï¼ˆæ¥è‡ªå›¾ç‰‡æˆ–è§†é¢‘ï¼‰ï¼Œå°†å…¶è½¬æ¢ä¸ºbfloat16ç±»å‹
    if "pixel_values" in inputs:
        inputs["pixel_values"] = inputs["pixel_values"].to(torch.bfloat16)

    # åˆå§‹åŒ–æµå¼è¾“å‡ºå™¨
    streamer = TextIteratorStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)
    # å‡†å¤‡ä¼ é€’ç»™æ¨¡å‹generateæ–¹æ³•çš„å‚æ•°
    generation_kwargs = {
        **inputs,
        **generation_config,
        "streamer": streamer,
    }

    # åˆ›å»ºå¹¶å¯åŠ¨ä¸€ä¸ªæ–°çº¿ç¨‹æ¥è¿è¡Œæ¨¡å‹ç”Ÿæˆï¼Œé¿å…é˜»å¡UI
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # åœ¨å¯¹è¯å†å²ä¸­æ·»åŠ ä¸€ä¸ªç©ºçš„åŠ©æ‰‹å›å¤ï¼Œç”¨äºåç»­å¡«å……
    messages.append({"role": "assistant", "content": ""})
    # è¿­ä»£æµå¼è¾“å‡ºå™¨ï¼Œè·å–ç”Ÿæˆçš„æ¯ä¸ªtoken
    for token in streamer:
        # å°†æ–°ç”Ÿæˆçš„tokenè¿½åŠ åˆ°æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯çš„å†…å®¹ä¸­
        messages[-1]['content'] += token
        # ä½¿ç”¨yieldè¿”å›æ›´æ–°åçš„å¯¹è¯å†å²ï¼ŒGradioä¼šè‡ªåŠ¨æ›´æ–°UI
        yield messages


# ä½¿ç”¨gr.Blocks()åˆ›å»ºä¸€ä¸ªGradioç•Œé¢
with gr.Blocks() as interface:
    # æ˜¾ç¤ºHTMLå¤´éƒ¨
    gr.HTML(HEADER)
    # åˆ›å»ºä¸€ä¸ªæ°´å¹³å¸ƒå±€
    with gr.Row():
        # åˆ›å»ºèŠå¤©æœºå™¨äººçª—å£
        chatbot = gr.Chatbot(type="messages", elem_id="chatbot", height=835)

        # åˆ›å»ºä¸€ä¸ªå‚ç›´å¸ƒå±€ï¼Œç”¨äºæ”¾ç½®è¾“å…¥å’Œé…ç½®ç»„ä»¶
        with gr.Column():
            # åˆ›å»ºä¸€ä¸ªé€‰é¡¹å¡å¸ƒå±€
            with gr.Tab(label="Input"):

                # åˆ›å»ºä¸€ä¸ªæ°´å¹³å¸ƒå±€ç”¨äºæ”¾ç½®è§†é¢‘å’Œå›¾ç‰‡ä¸Šä¼ ç»„ä»¶
                with gr.Row():
                    input_video = gr.Video(sources=["upload"], label="Upload Video")
                    input_image = gr.Image(sources=["upload"], type="filepath", label="Upload Image")

                # åˆ›å»ºæ–‡æœ¬è¾“å…¥æ¡†
                input_text = gr.Textbox(label="Input Text", placeholder="Type your message here and press enter to submit")

                # åˆ›å»ºæäº¤æŒ‰é’®
                submit_button = gr.Button("Generate")

                # åˆ›å»ºç¤ºä¾‹åŒºåŸŸï¼Œç‚¹å‡»åä¼šè‡ªåŠ¨å¡«å……è¾“å…¥
                gr.Examples(examples=[
                    [f"examples/è£…è¿è¿‡ç¨‹ç«ç¾.mp4"],
                    [f"examples/å·¥å‚çˆ†ç‡ƒ.mp4"],
                    [f"examples/å®¤å¤–é•åˆé‡‘ç²‰æœªçˆ†ç‚¸.mp4"],
                ], inputs=[input_video, input_text], label="Video examples")

            # åˆ›å»ºé…ç½®é€‰é¡¹å¡
            with gr.Tab(label="Configure"):
                # åˆ›å»ºå¯æŠ˜å çš„ç”Ÿæˆé…ç½®åŒºåŸŸ
                with gr.Accordion("Generation Config", open=True):
                    do_sample = gr.Checkbox(value=True, label="Do Sample")
                    temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.2, label="Temperature")
                    top_p = gr.Slider(minimum=0.0, maximum=1.0, value=0.9, label="Top P")
                    max_new_tokens = gr.Slider(minimum=0, maximum=4096, value=2048, step=1, label="Max New Tokens")

                # åˆ›å»ºå¯æŠ˜å çš„è§†é¢‘é…ç½®åŒºåŸŸ
                with gr.Accordion("Video Config", open=True):
                    fps = gr.Slider(minimum=0.0, maximum=10.0, value=1, label="FPS")
                    max_frames = gr.Slider(minimum=0, maximum=256, value=180, step=1, label="Max Frames")

    # ---- äº‹ä»¶ç»‘å®š ----
    # å°†è§†é¢‘ä¸Šä¼ ç»„ä»¶çš„changeäº‹ä»¶ç»‘å®šåˆ°_on_video_uploadå‡½æ•°
    input_video.change(_on_video_upload, [chatbot, input_video], [chatbot, input_video])
    # å°†å›¾ç‰‡ä¸Šä¼ ç»„ä»¶çš„changeäº‹ä»¶ç»‘å®šåˆ°_on_image_uploadå‡½æ•°
    input_image.change(_on_image_upload, [chatbot, input_image], [chatbot, input_image])
    # å°†æ–‡æœ¬è¾“å…¥æ¡†çš„submitäº‹ä»¶ï¼ˆæŒ‰å›è½¦ï¼‰ç»‘å®šåˆ°_on_text_submitå‡½æ•°
    input_text.submit(_on_text_submit, [chatbot, input_text], [chatbot, input_text])
    # å°†æäº¤æŒ‰é’®çš„clickäº‹ä»¶ç»‘å®šåˆ°_predictå‡½æ•°
    submit_button.click(
        _predict,
        [
            chatbot, input_text, do_sample, temperature, top_p, max_new_tokens,
            fps, max_frames
        ],
        [chatbot],
    )


# å¦‚æœè¯¥è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œåˆ™å¯åŠ¨Gradioç•Œé¢
if __name__ == "__main__":
    interface.launch()