import os
import sys
import yaml
import threading
import time
import traceback
import glob
import json
import ast
import re
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from typing import Dict, List, Optional, Generator, Tuple, Any
import gradio as gr

# VideoLLaMA3 æ ¸å¿ƒå¯¼å…¥
script_dir = Path(__file__).parent
root_dir = script_dir / 'VideoLLaMA3'
sys.path.insert(0, str(root_dir))

# å®šä¹‰Gradioç•Œé¢é¡¶éƒ¨çš„HTMLå†…å®¹
HEADER = ("""
<div style="display: flex; justify-content: center; align-items: center; text-align: center;">
  <div>
    <h1>FireVEDï¼šç«ç¾è§†é¢‘åº”æ€¥äº‹ä»¶æ£€æµ‹</h1>
    <h5 style="margin: 0;">ğŸ”¥Gradio Demo for Fire Video Emergency DetectionğŸ”¥</h5>
  </div>
</div>
""")
question = "<video>\nè¯·ä½ åˆ†æè§†é¢‘ç‰‡æ®µï¼Œåˆ¤æ–­å…¶ä¸­æ˜¯å¦å‘ç”Ÿäº†ä¸ç«ç¾æˆ–å…¶ä»–çªå‘æƒ…å†µç›¸å…³çš„åº”æ€¥äº‹ä»¶ã€‚è¯·æŒ‡å‡ºå„äº‹ä»¶å…¶åœ¨è§†é¢‘ç‰‡æ®µä¸­çš„æ—¶é—´èŒƒå›´ï¼Œå¹¶ç®€è¦æè¿°äº‹ä»¶å†…å®¹ã€‚è‹¥å­˜åœ¨å¤šä¸ªä¸åŒçš„äº‹ä»¶ï¼Œè¯·æŒ‰æ—¶é—´é¡ºåºè¾“å‡ºå¤šä¸ªjsonå•å…ƒï¼›å½“è§†é¢‘ä¸­å‘ç”Ÿæ–°çš„ã€æœ‰æ„ä¹‰çš„ã€ä¸ç«ç¾ç›¸å…³çš„äº‹ä»¶ï¼Œæˆ–å½“å‰äº‹ä»¶çŠ¶æ€å‘ç”Ÿæ˜¾è‘—å˜åŒ–æ—¶ï¼Œè¯·å¼€å§‹ä¸€ä¸ªæ–°çš„äº‹ä»¶æ®µã€‚äº‹ä»¶æ®µå¯ä»¥é‡å ã€‚\næ¯ä¸ªäº‹ä»¶è¾“å‡ºè¦æ±‚å¦‚ä¸‹ï¼ˆä¸¥æ ¼éµå¾ª JSON æ ¼å¼ï¼‰ï¼š\n{'emergency_exist': 'æ˜¯' æˆ– 'å¦',  // æ˜¯å¦å‘ç”Ÿåº”æ€¥äº‹ä»¶\n  'event_time': 'èµ·å§‹ç§’-ç»“æŸç§’',  // æ—¶é—´èŒƒå›´ï¼Œå•ä½ä¸ºç§’ï¼Œä¿ç•™ä¸€ä½å°æ•°\n  'event_des': 'äº‹ä»¶ç®€è¦æè¿°'  }\næ³¨æ„äº‹é¡¹ï¼šæ—¶é—´æ˜¯ç›¸å¯¹äºè¯¥è§†é¢‘ç‰‡æ®µçš„å±€éƒ¨æ—¶é—´ï¼ˆå³ç‰‡æ®µèµ·ç‚¹ä¸º 0sï¼‰ï¼›æ‰€æœ‰äº‹ä»¶çš„èŒƒå›´äº¤é›†è¦æ±‚è¦†ç›–å…¨æ—¶æ®µã€‚è¯·ä»…è¾“å‡ºç¬¦åˆä¸Šè¿°æ ¼å¼çš„ JSONã€‚å½“å‡ºç°æ­£å¸¸ã€å¼‚å¸¸äº‹ä»¶åˆ‡æ¢æ—¶æ‰åŒºåˆ†äº‹ä»¶è¾“å‡ºï¼Œè¿ç»­ç›¸åŒäº‹ä»¶åˆå¹¶ä¸ºä¸€ä¸ªäº‹ä»¶è¡¨è¿°ï¼Œäº‹ä»¶è¡¨è¿°å°½å¯èƒ½çŸ­ã€‚æœ‰çƒŸé›¾ä¹Ÿæ˜¯å¼‚å¸¸ã€‚"

try:
    from VideoLLaMA3.videollama3 import disable_torch_init, model_init, mm_infer
    from VideoLLaMA3.videollama3.mm_utils import load_video, load_images, torch
except ImportError as e:
    print(f"è­¦å‘Š: VideoLLaMA3 æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ VideoLLaMA3 ç›®å½•å­˜åœ¨ä¸”åŒ…å«å¿…è¦æ–‡ä»¶")

# =================== é…ç½®æ®µ ===================
class Config:
    """è½»é‡åŒ–é…ç½®ç®¡ç†"""
    DEFAULT_CONFIG = {
        "model": {
            "path": "model_ck",
            "device": "cuda",
            "torch_dtype": "bfloat16",
            "attn_implementation": "flash_attention_2"
        },
        "inference": {
            "fps": 1, # å°½é‡ä¸è¦é…ç½®
            "max_frames": 160,
            "modal": "video",
            "max_new_tokens": 180,
            "do_sample": False,
            "merge_size": 2,
            "timeout": 300  # 5åˆ†é’Ÿè¶…æ—¶
        }
    }

    def __init__(self, config_file: str = "config.yaml"):
        self.config_file = config_file
        self.config = self.load_config()

    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®ï¼Œå¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    return user_config
            else:
                # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
                self.save_config(self.DEFAULT_CONFIG)
                return self.DEFAULT_CONFIG
        except Exception as e:
            print(f"é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            return self.DEFAULT_CONFIG

    def save_config(self, config: Dict):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        except Exception as e:
            print(f"é…ç½®ä¿å­˜å¤±è´¥: {e}")

    def reload_config(self) -> str:
        """é‡æ–°åŠ è½½é…ç½®"""
        try:
            self.config = self.load_config()
            return "é…ç½®å·²é‡æ–°åŠ è½½"
        except Exception as e:
            return f"é…ç½®é‡è½½å¤±è´¥: {e}"

# =================== æ ¸å¿ƒä¸šåŠ¡æ®µ ===================
class VideoLLaMA3App:
    """æ ¸å¿ƒåº”ç”¨ç±» - é›†æˆæ‰€æœ‰åŠŸèƒ½"""

    def __init__(self):
        self.config = Config()
        self.model = None
        self.processor = None
        self.model_status = "æœªåŠ è½½"
        self.conversation_history = []
        self.current_task = None
        self.executor = ThreadPoolExecutor(max_workers=1)

    def load_model(self) -> Tuple[bool, str]:
        """åŸºäºinfer.pyçš„æ¨¡å‹åŠ è½½é€»è¾‘"""
        try:
            print("å¼€å§‹åŠ è½½æ¨¡å‹...")

            # ä¸¥æ ¼æŒ‰ç…§infer.pyçš„åˆå§‹åŒ–æ–¹å¼
            disable_torch_init()

            # ä½¿ç”¨infer.pyå®Œå…¨ç›¸åŒçš„åŠ è½½æ–¹å¼
            self.model, self.processor = model_init(
                model_path=self.config.config['model']['path'],
                torch_dtype=torch.bfloat16,
                attn_implementation=self.config.config['model']['attn_implementation']
            )

            device = self.config.config['model']['device']
            self.model = self.model.to(device)

            self.model_status = "å·²åŠ è½½"
            print("æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True, "æ¨¡å‹åŠ è½½æˆåŠŸ"

        except Exception as e:
            self.model_status = "åŠ è½½å¤±è´¥"
            error_msg = f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())
            return False, error_msg

    def unload_model(self) -> str:
        """å†…å­˜æ¸…ç†"""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None

            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            self.model_status = "æœªåŠ è½½"
            return "æ¨¡å‹å·²å¸è½½"
        except Exception as e:
            return f"æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}"

    def stream_inference(self, video_path: str, question: str) -> Generator[str, None, None]:
        """æµå¼
        - åŸºäºmm_infer"""
        if self.model is None or self.processor is None:
            yield "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹"
            return

        try:
            # çŠ¶æ€æ›´æ–°
            yield "ğŸ¬ æ­£åœ¨åŠ è½½è§†é¢‘..."

            # ä¸¥æ ¼æŒ‰ç…§infer.pyçš„å¤„ç†æ–¹å¼
            frames, timestamps = load_video(
                video_path,
                fps=self.config.config['inference']['fps'],
                max_frames=self.config.config['inference']['max_frames']
            )

            yield f"ğŸ“Š è§†é¢‘åŠ è½½å®Œæˆï¼Œå…± {len(frames)} å¸§"

            # æ„å»ºconversationï¼Œä¸¥æ ¼æŒ‰ç…§infer.pyçš„æ ¼å¼
            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "video", "timestamps": timestamps, "num_frames": len(frames)},
                        {"type": "text", "text": question},
                    ]
                }
            ]

            yield "ğŸ§  æ­£åœ¨é¢„å¤„ç†æ•°æ®..."

            # ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å¤„ç†å™¨å’Œåˆ†ææ–¹å¼
            modal = self.config.config['inference']['modal']

            # æ‰‹åŠ¨åº”ç”¨èŠå¤©æ¨¡æ¿ä»¥æ¶ˆé™¤UserWarning
            # å¤„ç†å™¨å†…éƒ¨æœŸæœ›ä¸€ä¸ªæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯å¯¹è¯åˆ—è¡¨
            text_input = self.processor.tokenizer.apply_chat_template(
                conversation,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = self.processor(
                images=[frames] if modal != "text" else None,
                text=text_input, # ä¼ å…¥é¢„æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
                merge_size=self.config.config['inference']['merge_size'],
                return_tensors="pt",
            )

            yield "âš¡ æ­£åœ¨åˆ†æä¸­..."

            # æ·»åŠ è¶…æ—¶æ§åˆ¶
            def run_inference():
                return mm_infer(
                    inputs,
                    model=self.model,
                    tokenizer=self.processor.tokenizer,
                    do_sample=self.config.config['inference']['do_sample'],
                    modal=modal,
                    max_new_tokens=self.config.config['inference']['max_new_tokens']
                )

            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œåˆ†æ
            self.current_task = self.executor.submit(run_inference)

            try:
                timeout = self.config.config['inference']['timeout']
                result = self.current_task.result(timeout=timeout)
                yield f"âœ… åˆ†æå®Œæˆ\n\n{result}"

                # æ·»åŠ åˆ°ä¼šè¯å†å²ï¼ˆä½¿ç”¨ç®€åŒ–çš„ç”¨æˆ·æ¶ˆæ¯ï¼‰
                self.add_message("user", "å¼€å§‹è§†é¢‘åˆ†æ")
                self.add_message("assistant", result)

            except TimeoutError:
                if self.current_task:
                    self.current_task.cancel()
                yield "â° åˆ†æè¶…æ—¶ï¼Œå·²ä¸­æ–­"

        except Exception as e:
            error_msg = f"âŒ åˆ†æå¤±è´¥: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())
            yield error_msg
        finally:
            self.current_task = None

    def interrupt_inference(self) -> str:
        """ä¸­æ–­åˆ†æ"""
        if self.current_task and not self.current_task.done():
            self.current_task.cancel()
            return "åˆ†æå·²ä¸­æ–­"
        return "æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„åˆ†æ"

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        self.conversation_history.append({
            "role": role,
            "content": content,
            "timestamp": time.time()
        })

    def clear_conversation(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
        return "å¯¹è¯å†å²å·²æ¸…ç©º"

    def get_model_info(self) -> str:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return "æ¨¡å‹æœªåŠ è½½"

        try:
            model_path = self.config.config['model']['path']
            device = self.config.config['model']['device']
            return f"æ¨¡å‹è·¯å¾„: {model_path}\nè®¾å¤‡: {device}\nçŠ¶æ€: {self.model_status}"
        except Exception as e:
            return f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}"

def parse_model_output(output_str: str) -> List[Dict]:
    """è§£ææ¨¡å‹è¾“å‡ºçš„å­—ç¬¦ä¸²ï¼Œæå–äº‹ä»¶å­—å…¸"""
    events = []
    # å»é™¤å¯èƒ½çš„Markdownä»£ç å—æ ‡è®°
    output_str = re.sub(r"```json\n?|```", "", output_str).strip()

    # å…¼å®¹å•å­—å…¸å’Œå¤šå­—å…¸åœºæ™¯
    # æŸ¥æ‰¾æ‰€æœ‰è¢«å¤§æ‹¬å·åŒ…å›´çš„éƒ¨åˆ†
    # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼å‡è®¾äº‹ä»¶æè¿°ä¸­ä¸åŒ…å«æœªè½¬ä¹‰çš„å¤§æ‹¬å·
    dict_strings = re.findall(r"\{[^{}]+\}", output_str.replace("\n", ""))

    if not dict_strings:
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•å°†æ•´ä¸ªå­—ç¬¦ä¸²ä½œä¸ºå•ä¸ªPythonå­—é¢é‡è§£æ
        try:
            # å°†Pythonå­—å…¸è½¬ä¸ºJSONå­—ç¬¦ä¸²ï¼Œç„¶åå†è§£æ
            # è¿™æ ·å¯ä»¥å¤„ç†å•å¼•å·ç­‰é—®é¢˜
            temp_list = ast.literal_eval(f"[{output_str}]")
            events.extend(temp_list)
        except (ValueError, SyntaxError) as e:
            print(f"è§£ææ•´ä¸ªå­—ç¬¦ä¸²å¤±è´¥: {e}")
            return [] # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return events

    for d_str in dict_strings:
        try:
            # ast.literal_eval æ›´å®‰å…¨ï¼Œèƒ½å¤„ç†Pythonçš„å­—å…¸æ ¼å¼ï¼ˆå¦‚å•å¼•å·ï¼‰
            event = ast.literal_eval(d_str)
            if isinstance(event, dict):
                events.append(event)
        except (ValueError, SyntaxError) as e:
            print(f"è§£æå•ä¸ªäº‹ä»¶å¤±è´¥: '{d_str}', é”™è¯¯: {e}")
            continue
    return events


def format_timeline_output(result_str: str) -> str:
    """å°†æ¨¡å‹è¾“å‡ºçš„äº‹ä»¶å­—ç¬¦ä¸²æ ¼å¼åŒ–ä¸ºåŒ…å«çŠ¶æ€å’Œæ—¶é—´è½´çš„HTML"""
    events = parse_model_output(result_str)

    if not events:
        status_md = "## <p style='text-align:center;'>âšª åˆ†æå®Œæˆï¼Œæœªæ£€æµ‹åˆ°æœ‰æ•ˆäº‹ä»¶ä¿¡æ¯ã€‚</p>"
        timeline_md = "<p>æœªæ£€æµ‹åˆ°æœ‰æ•ˆäº‹ä»¶ä¿¡æ¯ã€‚</p>"
        return status_md + timeline_md

    # a. æ€»ä½“çŠ¶æ€
    is_emergency = any(item.get('emergency_exist') == 'æ˜¯' for item in events)
    if is_emergency:
        status_md = "## <p style='color:#FF4136; text-align:center;'>ğŸ”´ æ£€æµ‹åˆ°åº”æ€¥äº‹ä»¶ï¼</p>"
    else:
        status_md = "## <p style='color:#2ECC40; text-align:center;'>ğŸŸ¢ æœªå‘ç°å¼‚å¸¸</p>"

    # b. äº‹ä»¶æ—¶é—´è½´
    timeline_md = ""
    for i, event in enumerate(events, 1):
        if event.get('emergency_exist') == 'æ˜¯':
            style = "border-left: 5px solid #FF4136; padding-left: 15px; margin: 20px 0;"
            icon = "ğŸ”¥"
            level = "è­¦å‘Š"
        else:
            style = "border-left: 5px solid #2ECC40; padding-left: 15px; margin: 20px 0;"
            icon = "âœ…"
            level = "æ­£å¸¸"

        timeline_md += f"""
        <div style='{style}'>
            <p style='font-size: 1.1em; font-weight: bold; margin:0;'>{icon} äº‹ä»¶ {i}: {level}</p>
            <p style='margin: 5px 0;'><strong>æ—¶é—´èŒƒå›´:</strong> {event.get('event_time', 'N/A')}</p>
            <p style='margin: 5px 0;'><strong>äº‹ä»¶æè¿°:</strong> {event.get('event_des', 'N/A')}</p>
        </div>
        """
    return status_md + timeline_md


# =================== UIæ®µ ===================
def create_gradio_interface(app: VideoLLaMA3App) -> gr.Blocks:
    """åˆ›å»ºç®€æ´é«˜æ•ˆçš„UIç•Œé¢"""

    gr.set_static_paths(paths=[Path.cwd().absolute()/"assets"])

    with gr.Blocks(
        title="FireVED: Fire Video Emergency Detection",
        theme=gr.themes.Soft()
    ) as demo:
        gr.HTML(HEADER)

        # çŠ¶æ€å˜é‡
        conversation_state = gr.State([])

        with gr.Row():
            # å·¦ä¾§æ§åˆ¶åŒºåŸŸ (scale=2)
            with gr.Column(scale=2):
                with gr.Tab(label="Input"):
                    # è¾“å…¥åŒºåŸŸ
                    video_input = gr.Video(
                        label="ä¸Šä¼ è§†é¢‘",
                        container=True,
                        interactive=True,
                        height=400,
                        sources=["upload"],
                    )

                    # æŒ‰é’®ç»„
                    with gr.Row():
                        submit_btn = gr.Button("æ£€æµ‹", variant="primary")
                        interrupt_btn = gr.Button("ä¸­æ–­", variant="stop")

                    # çŠ¶æ€æ˜¾ç¤º
                    status_display = gr.Textbox(
                        label="ç³»ç»ŸçŠ¶æ€",
                        value="å°±ç»ª",
                        interactive=False,
                        max_lines=2
                    )

                    # å¯¹è¯æ§åˆ¶
                    with gr.Row():
                        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", size="sm")

                    # è§†é¢‘æ¡ˆä¾‹
                    gr.Markdown("### Examples")

                    # æ£€ç´¢ examples ç›®å½•ä¸‹æ‰€æœ‰ mp4 æ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹æ–‡ä»¶åˆ—è¡¨
                    examples_dir = "examples"
                    example_videos = glob.glob(os.path.join(examples_dir, "*.mp4"))
                    examples_data = []
                    for video_path in example_videos:
                        examples_data.append([
                            video_path
                        ])

                    if examples_data:
                        examples = gr.Examples(
                            examples=examples_data,
                            inputs=[video_input],
                            label="ç‚¹å‡»é€‰æ‹©ç¤ºä¾‹"
                        )
                    else:
                        gr.Markdown("*æ²¡æœ‰æ‰¾åˆ°ç¤ºä¾‹è§†é¢‘æ–‡ä»¶*")

                with gr.Tab(label="Configure"):
                    # æ¨¡å‹ç®¡ç†
                    gr.Markdown("### æ¨¡å‹ç®¡ç†")
                    model_status = gr.Textbox(
                        label="æ¨¡å‹çŠ¶æ€",
                        value="æœªåŠ è½½",
                        interactive=False
                    )

                    with gr.Row():
                        load_btn = gr.Button("åŠ è½½æ¨¡å‹", size="sm")
                        unload_btn = gr.Button("å¸è½½æ¨¡å‹", size="sm")

                    # é…ç½®ç®¡ç†
                    gr.Markdown("### é…ç½®ç®¡ç†")
                    config_display = gr.Code(
                        value=yaml.dump(app.config.config, default_flow_style=False),
                        language="yaml",
                        label="å½“å‰é…ç½®",
                        lines=8
                    )

                    reload_config_btn = gr.Button("é‡è½½é…ç½®", size="sm")

            # å³ä¾§èŠå¤©åŒºåŸŸ
            with gr.Column(scale=2):
                with gr.Blocks():
                    chatbot = gr.Chatbot(
                        type="messages",
                        height=514,
                        bubble_full_width=False,
                        show_copy_button=True,
                        label="å¯¹è¯å†å²"
                    )
                with gr.Blocks():
                    timeline_output = gr.Markdown("### <p style='text-align:center;'>âšª ç­‰å¾…åˆ†æ...</p>", container=True)

        # äº‹ä»¶å¤„ç†å‡½æ•°
        def handle_submit(video, history):
            """å¤„ç†æäº¤äº‹ä»¶"""
            if not video:
                gr.Warning("è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶")
                return history, history, "### <p style='text-align:center;'>âšª ç­‰å¾…åˆ†æ...</p>", "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶"

            if app.model_status != "å·²åŠ è½½":
                gr.Warning("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹")
                return history, history, "### <p style='text-align:center;'>âšª ç­‰å¾…åˆ†æ...</p>", "è¯·å…ˆåŠ è½½æ¨¡å‹"

            # æ¸…ç©ºä¸Šä¸€æ¬¡çš„ç»“æœ
            initial_timeline_md = "## <p style='text-align:center;'>â³ åˆ†æè¿›è¡Œä¸­...</p>"

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ˜¾ç¤ºç®€åŒ–ä¿¡æ¯ï¼‰
            user_msg = {"role": "user", "content": "è§†é¢‘åˆ†æä»»åŠ¡å·²æäº¤ï¼Œè¯·ç¨å€™..."}
            history.append(user_msg)

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯å ä½ç¬¦
            assistant_msg = {"role": "assistant", "content": "â³ å¼€å§‹åˆ†æï¼Œè¯·ç¨å€™..."}
            history.append(assistant_msg)
            yield history, history, initial_timeline_md, "åˆ†æä¸­..."

            final_result = ""
            # æµå¼åˆ†æï¼ˆä½¿ç”¨å†…éƒ¨é—®é¢˜ï¼‰
            for partial_response in app.stream_inference(video, question):
                history[-1]["content"] = partial_response
                final_result = partial_response
                yield history, history, initial_timeline_md, "åˆ†æä¸­..."

            # åˆ†æå®Œæˆåï¼Œæ ¼å¼åŒ–æœ€ç»ˆç»“æœ
            prefix = "âœ… åˆ†æå®Œæˆ\n\n"
            if final_result.startswith(prefix):
                json_part = final_result[len(prefix):].strip()

                # 1. ä¸º timeline_output ç”ŸæˆHTML
                combined_md = format_timeline_output(json_part)

                # 2. ä¸ºChatbotå‡†å¤‡æ ¼å¼åŒ–çš„JSON
                try:
                    events = parse_model_output(json_part)
                    # ç¡®ä¿å³ä½¿åªæœ‰ä¸€ä¸ªäº‹ä»¶ï¼Œä¹Ÿä»¥åˆ—è¡¨å½¢å¼å‡ºç°
                    if isinstance(events, dict):
                       events = [events]
                    pretty_json = json.dumps(events, indent=2, ensure_ascii=False)
                    chatbot_content = f"```json\n{pretty_json}\n```"
                except Exception:
                    chatbot_content = f"```json\n{json_part}\n```"

                history[-1]["content"] = chatbot_content
                yield history, history, combined_md, "åˆ†æå®Œæˆ"

            else:
                # å¯¹äºé”™è¯¯æˆ–è¶…æ—¶ç­‰æƒ…å†µï¼Œç›´æ¥æ˜¾ç¤º
                history[-1]["content"] = final_result
                error_md = f"## <p style='color:red;text-align:center;'>âŒ åˆ†æå¤±è´¥æˆ–è¶…æ—¶</p><p>é”™è¯¯ä¿¡æ¯: {final_result}</p>"
                yield history, history, error_md, "åˆ†æå¤±è´¥"

        def handle_load_model():
            """å¤„ç†æ¨¡å‹åŠ è½½"""
            success, message = app.load_model()
            status = "å·²åŠ è½½" if success else "åŠ è½½å¤±è´¥"
            info = app.get_model_info()
            return status, info, message

        def handle_unload_model():
            """å¤„ç†æ¨¡å‹å¸è½½"""
            message = app.unload_model()
            info = app.get_model_info()
            return "æœªåŠ è½½", info, message

        def handle_interrupt():
            """å¤„ç†ä¸­æ–­äº‹ä»¶"""
            message = app.interrupt_inference()
            return message

        def handle_clear_conversation(history):
            """å¤„ç†æ¸…ç©ºå¯¹è¯"""
            app.clear_conversation()
            return [], [], "### <p style='text-align:center;'>âšª ç­‰å¾…åˆ†æ...</p>", "å¯¹è¯å†å²å·²æ¸…ç©º"

        def handle_reload_config():
            """å¤„ç†é…ç½®é‡è½½"""
            message = app.config.reload_config()
            new_config = yaml.dump(app.config.config, default_flow_style=False)
            return new_config, message

        # ç»‘å®šäº‹ä»¶
        submit_btn.click(
            fn=handle_submit,
            inputs=[video_input, conversation_state],
            outputs=[chatbot, conversation_state, timeline_output, status_display]
        )

        load_btn.click(
            fn=handle_load_model,
            outputs=[model_status, config_display, status_display]
        )

        unload_btn.click(
            fn=handle_unload_model,
            outputs=[model_status, config_display, status_display]
        )

        interrupt_btn.click(
            fn=handle_interrupt,
            outputs=[status_display]
        )

        clear_btn.click(
            fn=handle_clear_conversation,
            inputs=[conversation_state],
            outputs=[chatbot, conversation_state, timeline_output, status_display]
        )

        reload_config_btn.click(
            fn=handle_reload_config,
            outputs=[config_display, status_display]
        )

    return demo

# =================== ä¸»ç¨‹åºæ®µ ===================
try:
    print("ğŸš€ å¯åŠ¨ VideoLLaMA3 Gradio åº”ç”¨...")

    # åˆ›å»ºåº”ç”¨å®ä¾‹
    app = VideoLLaMA3App()

    # åˆ›å»ºç•Œé¢
    demo = create_gradio_interface(app)

except Exception as e:
    print(f"âŒ åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
    print(traceback.format_exc())

if __name__ == "__main__":
    # å¯åŠ¨åº”ç”¨
    print("ğŸŒ å¯åŠ¨Webç•Œé¢...")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        show_api=False,
        debug=False,
        quiet=False,
        allowed_paths=["assets"]
    )