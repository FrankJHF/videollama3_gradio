import os
import sys
import yaml
import threading
import time
import traceback
import glob
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from typing import Dict, List, Optional, Generator, Tuple, Any
import gradio as gr

# VideoLLaMA3 æ ¸å¿ƒå¯¼å…¥
script_dir = Path(__file__).parent
root_dir = script_dir / 'VideoLLaMA3'
sys.path.insert(0, str(root_dir))

# å®šä¹‰Gradioç•Œé¢é¡¶éƒ¨çš„HTMLå†…å®¹
HEADER = ("""
<div style="display: flex; justify-content: center; align-items: center; text-align: center;">
  <div>
    <h1>FireVEDï¼šç«ç¾è§†é¢‘åº”æ€¥äº‹ä»¶æ£€æµ‹</h1>
    <h5 style="margin: 0;">ğŸ”¥Gradio Demo for Fire Video Emergency DetectionğŸ”¥</h5>
  </div>
</div>
""")
question = (
        "è¯·ä½ åˆ†æè§†é¢‘ç‰‡æ®µï¼Œåˆ¤æ–­å…¶ä¸­æ˜¯å¦å‘ç”Ÿäº†ä¸ç«ç¾æˆ–å…¶ä»–çªå‘æƒ…å†µç›¸å…³çš„åº”æ€¥äº‹ä»¶ã€‚"
        "è¯·æŒ‡å‡ºå„äº‹ä»¶å…¶åœ¨è§†é¢‘ç‰‡æ®µä¸­çš„æ—¶é—´èŒƒå›´ï¼Œå¹¶ç®€è¦æè¿°äº‹ä»¶å†…å®¹ã€‚"
        "è‹¥å­˜åœ¨å¤šä¸ªä¸åŒçš„äº‹ä»¶ï¼Œè¯·æŒ‰æ—¶é—´é¡ºåºè¾“å‡ºå¤šä¸ªjsonå•å…ƒï¼›"
        "å½“è§†é¢‘ä¸­å‘ç”Ÿæ–°çš„ã€æœ‰æ„ä¹‰çš„ã€ä¸ç«ç¾ç›¸å…³çš„äº‹ä»¶ï¼Œæˆ–å½“å‰äº‹ä»¶çŠ¶æ€å‘ç”Ÿæ˜¾è‘—å˜åŒ–æ—¶ï¼Œ"
        "è¯·å¼€å§‹ä¸€ä¸ªæ–°çš„äº‹ä»¶æ®µã€‚äº‹ä»¶æ®µå¯ä»¥é‡å ã€‚\n"
        "æ¯ä¸ªäº‹ä»¶è¾“å‡ºè¦æ±‚å¦‚ä¸‹ï¼ˆä¸¥æ ¼éµå¾ª JSON æ ¼å¼ï¼‰ï¼š\n"
        "{'emergency_exist': 'æ˜¯' æˆ– 'å¦',  // æ˜¯å¦å‘ç”Ÿåº”æ€¥äº‹ä»¶\n"
        "  'event_time': 'èµ·å§‹ç§’-ç»“æŸç§’',  // æ—¶é—´èŒƒå›´ï¼Œå•ä½ä¸ºç§’ï¼Œä¿ç•™ä¸€ä½å°æ•°\n"
        "  'event_des': 'äº‹ä»¶ç®€è¦æè¿°'  }\n"
        "æ³¨æ„äº‹é¡¹ï¼šæ—¶é—´æ˜¯ç›¸å¯¹äºè¯¥è§†é¢‘ç‰‡æ®µçš„å±€éƒ¨æ—¶é—´ï¼ˆå³ç‰‡æ®µèµ·ç‚¹ä¸º 0sï¼‰ï¼›"
        "æ‰€æœ‰äº‹ä»¶çš„èŒƒå›´äº¤é›†è¦æ±‚è¦†ç›–å…¨æ—¶æ®µã€‚è¯·ä»…è¾“å‡ºç¬¦åˆä¸Šè¿°æ ¼å¼çš„ JSONã€‚"
        "å½“å‡ºç°æ­£å¸¸ã€å¼‚å¸¸äº‹ä»¶åˆ‡æ¢æ—¶æ‰åŒºåˆ†äº‹ä»¶è¾“å‡ºï¼Œè¿ç»­ç›¸åŒäº‹ä»¶åˆå¹¶ä¸ºä¸€ä¸ªäº‹ä»¶è¡¨è¿°ï¼Œ"
        "äº‹ä»¶è¡¨è¿°å°½å¯èƒ½çŸ­ã€‚æœ‰çƒŸé›¾ä¹Ÿæ˜¯å¼‚å¸¸ã€‚"
    )

try:
    from VideoLLaMA3.videollama3 import disable_torch_init, model_init, mm_infer
    from VideoLLaMA3.videollama3.mm_utils import load_video, load_images, torch
except ImportError as e:
    print(f"è­¦å‘Š: VideoLLaMA3 æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ VideoLLaMA3 ç›®å½•å­˜åœ¨ä¸”åŒ…å«å¿…è¦æ–‡ä»¶")

# =================== é…ç½®æ®µ ===================
class Config:
    """è½»é‡åŒ–é…ç½®ç®¡ç†"""
    DEFAULT_CONFIG = {
        "model": {
            "path": "model_ck",
            "device": "cuda",
            "torch_dtype": "bfloat16",
            "attn_implementation": "flash_attention_2"
        },
        "inference": {
            "fps": 1,
            "max_frames": 180,
            "modal": "video",
            "max_new_tokens": 4096,
            "do_sample": False,
            "merge_size": 2,
            "timeout": 300  # 5åˆ†é’Ÿè¶…æ—¶
        }
    }

    def __init__(self, config_file: str = "config.yaml"):
        self.config_file = config_file
        self.config = self.load_config()

    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®ï¼Œå¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    return self.merge_config(user_config)
            else:
                # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
                self.save_config(self.DEFAULT_CONFIG)
                return self.DEFAULT_CONFIG
        except Exception as e:
            print(f"é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            return self.DEFAULT_CONFIG

    def merge_config(self, user_config: Dict) -> Dict:
        """åˆå¹¶ç”¨æˆ·é…ç½®å’Œé»˜è®¤é…ç½®"""
        merged = self.DEFAULT_CONFIG.copy()
        for key, value in user_config.items():
            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
                merged[key].update(value)
            else:
                merged[key] = value
        return merged

    def save_config(self, config: Dict):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        except Exception as e:
            print(f"é…ç½®ä¿å­˜å¤±è´¥: {e}")

    def reload_config(self) -> str:
        """é‡æ–°åŠ è½½é…ç½®"""
        try:
            self.config = self.load_config()
            return "é…ç½®å·²é‡æ–°åŠ è½½"
        except Exception as e:
            return f"é…ç½®é‡è½½å¤±è´¥: {e}"

# =================== æ ¸å¿ƒä¸šåŠ¡æ®µ ===================
class VideoLLaMA3App:
    """æ ¸å¿ƒåº”ç”¨ç±» - é›†æˆæ‰€æœ‰åŠŸèƒ½"""

    def __init__(self):
        self.config = Config()
        self.model = None
        self.processor = None
        self.model_status = "æœªåŠ è½½"
        self.conversation_history = []
        self.current_task = None
        self.executor = ThreadPoolExecutor(max_workers=1)

    def load_model(self) -> Tuple[bool, str]:
        """åŸºäºinfer.pyçš„æ¨¡å‹åŠ è½½é€»è¾‘"""
        try:
            print("å¼€å§‹åŠ è½½æ¨¡å‹...")

            # ä¸¥æ ¼æŒ‰ç…§infer.pyçš„åˆå§‹åŒ–æ–¹å¼
            disable_torch_init()

            # ä½¿ç”¨infer.pyå®Œå…¨ç›¸åŒçš„åŠ è½½æ–¹å¼
            self.model, self.processor = model_init(
                model_path=self.config.config['model']['path'],
                torch_dtype=torch.bfloat16,
                attn_implementation=self.config.config['model']['attn_implementation']
            )

            device = self.config.config['model']['device']
            self.model = self.model.to(device)

            self.model_status = "å·²åŠ è½½"
            print("æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True, "æ¨¡å‹åŠ è½½æˆåŠŸ"

        except Exception as e:
            self.model_status = "åŠ è½½å¤±è´¥"
            error_msg = f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())
            return False, error_msg

    def unload_model(self) -> str:
        """å†…å­˜æ¸…ç†"""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None

            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            self.model_status = "æœªåŠ è½½"
            return "æ¨¡å‹å·²å¸è½½"
        except Exception as e:
            return f"æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}"

    def stream_inference(self, video_path: str, question: str) -> Generator[str, None, None]:
        """æµå¼
        - åŸºäºmm_infer"""
        if self.model is None or self.processor is None:
            yield "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹"
            return

        try:
            # çŠ¶æ€æ›´æ–°
            yield "ğŸ¬ æ­£åœ¨åŠ è½½è§†é¢‘..."

            # ä¸¥æ ¼æŒ‰ç…§infer.pyçš„å¤„ç†æ–¹å¼
            frames, timestamps = load_video(
                video_path,
                fps=self.config.config['inference']['fps'],
                max_frames=self.config.config['inference']['max_frames']
            )

            yield f"ğŸ“Š è§†é¢‘åŠ è½½å®Œæˆï¼Œå…± {len(frames)} å¸§"

            # æ„å»ºconversationï¼Œä¸¥æ ¼æŒ‰ç…§infer.pyçš„æ ¼å¼
            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "video", "timestamps": timestamps, "num_frames": len(frames)},
                        {"type": "text", "text": question},
                    ]
                }
            ]

            yield "ğŸ§  æ­£åœ¨é¢„å¤„ç†æ•°æ®..."

            # ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å¤„ç†å™¨å’Œåˆ†ææ–¹å¼
            modal = self.config.config['inference']['modal']
            inputs = self.processor(
                images=[frames] if modal != "text" else None,
                text=conversation,
                merge_size=self.config.config['inference']['merge_size'],
                return_tensors="pt",
            )

            yield "âš¡ æ­£åœ¨åˆ†æä¸­..."

            # æ·»åŠ è¶…æ—¶æ§åˆ¶
            def run_inference():
                return mm_infer(
                    inputs,
                    model=self.model,
                    tokenizer=self.processor.tokenizer,
                    do_sample=self.config.config['inference']['do_sample'],
                    modal=modal,
                    max_new_tokens=self.config.config['inference']['max_new_tokens']
                )

            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œåˆ†æ
            self.current_task = self.executor.submit(run_inference)

            try:
                timeout = self.config.config['inference']['timeout']
                result = self.current_task.result(timeout=timeout)
                yield f"âœ… åˆ†æå®Œæˆ\n\n{result}"

                # æ·»åŠ åˆ°ä¼šè¯å†å²ï¼ˆä½¿ç”¨ç®€åŒ–çš„ç”¨æˆ·æ¶ˆæ¯ï¼‰
                self.add_message("user", "å¼€å§‹è§†é¢‘åˆ†æ")
                self.add_message("assistant", result)

            except TimeoutError:
                if self.current_task:
                    self.current_task.cancel()
                yield "â° åˆ†æè¶…æ—¶ï¼Œå·²ä¸­æ–­"

        except Exception as e:
            error_msg = f"âŒ åˆ†æå¤±è´¥: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())
            yield error_msg
        finally:
            self.current_task = None

    def interrupt_inference(self) -> str:
        """ä¸­æ–­åˆ†æ"""
        if self.current_task and not self.current_task.done():
            self.current_task.cancel()
            return "åˆ†æå·²ä¸­æ–­"
        return "æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„åˆ†æ"

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        self.conversation_history.append({
            "role": role,
            "content": content,
            "timestamp": time.time()
        })

    def clear_conversation(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
        return "å¯¹è¯å†å²å·²æ¸…ç©º"

    def get_model_info(self) -> str:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return "æ¨¡å‹æœªåŠ è½½"

        try:
            model_path = self.config.config['model']['path']
            device = self.config.config['model']['device']
            return f"æ¨¡å‹è·¯å¾„: {model_path}\nè®¾å¤‡: {device}\nçŠ¶æ€: {self.model_status}"
        except Exception as e:
            return f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}"

# =================== UIæ®µ ===================
def create_gradio_interface(app: VideoLLaMA3App) -> gr.Blocks:
    """åˆ›å»ºç®€æ´é«˜æ•ˆçš„UIç•Œé¢"""

    gr.set_static_paths(paths=[Path.cwd().absolute()/"assets"])

    with gr.Blocks(
        title="FireVED: Fire Video Emergency Detection",
        theme=gr.themes.Default()
    ) as demo:
        gr.HTML(HEADER)

        # çŠ¶æ€å˜é‡
        conversation_state = gr.State([])

        with gr.Row():
            # å³ä¾§æ§åˆ¶åŒºåŸŸ (scale=2)
            with gr.Column(scale=2):
                with gr.Tab(label="Input"):
                    # è¾“å…¥åŒºåŸŸ
                    video_input = gr.Video(
                        label="ä¸Šä¼ è§†é¢‘",
                        container=True,
                        interactive=True,
                        height=400,
                        sources=["upload"],
                    )

                    # æŒ‰é’®ç»„
                    with gr.Row():
                        submit_btn = gr.Button("æ£€æµ‹", variant="primary")
                        interrupt_btn = gr.Button("ä¸­æ–­", variant="stop")

                    # è§†é¢‘æ¡ˆä¾‹
                    gr.Markdown("### Examples")

                    # æ£€ç´¢ examples ç›®å½•ä¸‹æ‰€æœ‰ mp4 æ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹æ–‡ä»¶åˆ—è¡¨
                    examples_dir = "examples"
                    example_videos = glob.glob(os.path.join(examples_dir, "*.mp4"))
                    examples_data = []
                    for video_path in example_videos:
                        examples_data.append([
                            video_path
                        ])

                    if examples_data:
                        examples = gr.Examples(
                            examples=examples_data,
                            inputs=[video_input],
                            label="ç‚¹å‡»é€‰æ‹©ç¤ºä¾‹"
                        )
                    else:
                        gr.Markdown("*æ²¡æœ‰æ‰¾åˆ°ç¤ºä¾‹è§†é¢‘æ–‡ä»¶*")

                with gr.Tab(label="Configure"):
                    # æ¨¡å‹ç®¡ç†
                    gr.Markdown("### æ¨¡å‹ç®¡ç†")
                    model_status = gr.Textbox(
                        label="æ¨¡å‹çŠ¶æ€",
                        value="æœªåŠ è½½",
                        interactive=False
                    )

                    with gr.Row():
                        load_btn = gr.Button("åŠ è½½æ¨¡å‹", size="sm")
                        unload_btn = gr.Button("å¸è½½æ¨¡å‹", size="sm")

                    # é…ç½®ç®¡ç†
                    gr.Markdown("### é…ç½®ç®¡ç†")
                    config_display = gr.Code(
                        value=yaml.dump(app.config.config, default_flow_style=False),
                        language="yaml",
                        label="å½“å‰é…ç½®",
                        lines=8
                    )

                    reload_config_btn = gr.Button("é‡è½½é…ç½®", size="sm")

            # å·¦ä¾§èŠå¤©åŒºåŸŸ
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(
                    type="messages",
                    height=570,
                    bubble_full_width=False,
                    show_copy_button=True,
                    label="å¯¹è¯å†å²"
                )

                # çŠ¶æ€æ˜¾ç¤º
                status_display = gr.Textbox(
                    label="ç³»ç»ŸçŠ¶æ€",
                    value="å°±ç»ª",
                    interactive=False,
                    max_lines=2
                )

                # å¯¹è¯æ§åˆ¶
                with gr.Row():
                    clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", size="sm")

        # äº‹ä»¶å¤„ç†å‡½æ•°
        def handle_submit(video, history):
            """å¤„ç†æäº¤äº‹ä»¶"""
            if not video:
                return history, history, "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶"

            if app.model_status != "å·²åŠ è½½":
                return history, history, "è¯·å…ˆåŠ è½½æ¨¡å‹"

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ˜¾ç¤ºç®€åŒ–ä¿¡æ¯ï¼‰
            user_msg = {"role": "user", "content": "å¼€å§‹è§†é¢‘åˆ†æ"}
            history.append(user_msg)

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯å ä½ç¬¦
            assistant_msg = {"role": "assistant", "content": ""}
            history.append(assistant_msg)

            # æµå¼åˆ†æï¼ˆä½¿ç”¨å†…éƒ¨é—®é¢˜ï¼‰
            for partial_response in app.stream_inference(video, question):
                history[-1]["content"] = partial_response
                yield history, history, f"åˆ†æä¸­..."

            return history, history, "åˆ†æå®Œæˆ"

        def handle_load_model():
            """å¤„ç†æ¨¡å‹åŠ è½½"""
            success, message = app.load_model()
            status = "å·²åŠ è½½" if success else "åŠ è½½å¤±è´¥"
            info = app.get_model_info()
            return status, info, message

        def handle_unload_model():
            """å¤„ç†æ¨¡å‹å¸è½½"""
            message = app.unload_model()
            info = app.get_model_info()
            return "æœªåŠ è½½", info, message

        def handle_interrupt():
            """å¤„ç†ä¸­æ–­äº‹ä»¶"""
            message = app.interrupt_inference()
            return message

        def handle_clear_conversation(history):
            """å¤„ç†æ¸…ç©ºå¯¹è¯"""
            app.clear_conversation()
            return [], [], "å¯¹è¯å†å²å·²æ¸…ç©º"

        def handle_reload_config():
            """å¤„ç†é…ç½®é‡è½½"""
            message = app.config.reload_config()
            new_config = yaml.dump(app.config.config, default_flow_style=False)
            return new_config, message

        # ç»‘å®šäº‹ä»¶
        submit_btn.click(
            fn=handle_submit,
            inputs=[video_input, conversation_state],
            outputs=[chatbot, conversation_state, status_display]
        )

        load_btn.click(
            fn=handle_load_model,
            outputs=[model_status, config_display, status_display]
        )

        unload_btn.click(
            fn=handle_unload_model,
            outputs=[model_status, config_display, status_display]
        )

        interrupt_btn.click(
            fn=handle_interrupt,
            outputs=[status_display]
        )

        clear_btn.click(
            fn=handle_clear_conversation,
            inputs=[conversation_state],
            outputs=[chatbot, conversation_state, status_display]
        )

        reload_config_btn.click(
            fn=handle_reload_config,
            outputs=[config_display, status_display]
        )

    return demo

# =================== ä¸»ç¨‹åºæ®µ ===================
try:
    print("ğŸš€ å¯åŠ¨ VideoLLaMA3 Gradio åº”ç”¨...")

    # åˆ›å»ºåº”ç”¨å®ä¾‹
    app = VideoLLaMA3App()

    # åˆ›å»ºç•Œé¢
    demo = create_gradio_interface(app)

except Exception as e:
    print(f"âŒ åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
    print(traceback.format_exc())

if __name__ == "__main__":
    # å¯åŠ¨åº”ç”¨
    print("ğŸŒ å¯åŠ¨Webç•Œé¢...")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        show_error=True,
        show_api=False,
        debug=False,
        quiet=False,
        allowed_paths=["assets"]
    )